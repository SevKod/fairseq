
# Tutorial : Pretraining a WavLM-Base model from scratch

This step-by-step tutorial will allow you to pretrain the self-supervised model [WavLM](https://arxiv.org/pdf/2110.13900.pdf) from scratch using your own audio dataset. Let's buckle up and get right into it !

**WARNING** : The pretraining involves the use of Facebook's AI library fairseq conjointly with Microsoft's Unispeech implementation of the WavLM model. The combination of both libraries is still experimental. 

Prerequisites : 

- The [altered fairseq library](https://github.com/SevKod/fairseq). It contains the [WavLM model](https://github.com/SevKod/fairseq/blob/main/fairseq/models/wavlm/wavlm.py), its [criterion](https://github.com/SevKod/fairseq/blob/main/fairseq/criterions/wavlm_criterion.py) (which mainly serves to compute metrics like the loss and the number of good/bad predicted logits), the [noisy/overlapped speech simulation](https://github.com/SevKod/fairseq/blob/main/fairseq/tasks/utterance_mixing_pretraining.py) done on the dataset, and the [Gated Relative Position Bias](https://github.com/SevKod/fairseq/blob/5b088c7e7d04e0c1a7696bda5ff882c5e600ef05/fairseq/modules/wavlm/modules.py#L521C19-L521C19).

- The tutorial assumes your audio files are sampled at **16000 Hz**, and are of duration **higher** than at least **25 ms**. Furthermore, the pretraining was tested for audio files carrying only a single extension type : Either the **.flac** or **.wav** format. The pretraining might behave differently (or even crash) if these conditions are not respected. Make sure to uniformize your dataset by either over/down sampling your files (to match the 16 kHz), and/or converting them to the same format. 


The location of your audio files do not really matter AS LONG as they can be retrieved from an unique path. 
**Ex :**
 * **unique/location/to/audio**
    * **dir1**
        * audio11.wav
        * audio12.wav
        * audio13.wav
    * **dir2**
        * audio21.wav
        * **dir3**
            * audio311.wav

To make the tutorial as clear as possible, an example will be provided at each step. It is recommended to use a bash script since we will be using various scripts to prepare the pretraining.

#### Bash script example

```python
# Declare variables

tsv_dir=data/
cluster_nb=500
...

# To call variables as input to a .py
python random_script.py ${tsv_dir} ${cluster_nb}

# The above will result in executing : python random_script.py data/ 500
```
Finally, using absolute paths for specifying folders or files location might simplifly a lot the application of this tutorial.

All the scripts used to prepare data for pretraining are available in this directory (in [simple_kmeans](https://github.com/SevKod/fairseq/tree/main/examples/hubert/simple_kmeans))

## STEP 1 : Structure your dataset

The first step is to map your audio files and sort the dataset. The goal is to generate a sheet that will contain the path to all your files (whether they are in subfolders or not), fetch their length, as well as splitting the dataset into a training and validation set.
This is done automatically by using the ["wav2vec_manifest.py"](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/examples/wav2vec/wav2vec_manifest.py#L4). 

The script will automatically generate a **.tsv** file for both the training (**train.tsv**) and the validation (**valid.tsv**) set.

User has to specify : 
- The location of the repertory containing all his folders/subfolders which contains audio (the **unique/location/to/audio** path mentioned earlier)
- The output location of the **.tsv** files generated by the script
- The audio format of his dataset (**.wav** or **.flac**)
- The training/validation ratio (between **0** and **1**).

Then run :
```python
data_location=unique/location/to/audio
tsv_dir=output/location/of/tsv
format=wav_or_flac
ratio=0.01 

python location/to/wav2vec_manifest.py ${data_location} --dest ${tsv_dir} --ext ${format} --valid-percent ${ratio}

```
## STEP 2 : Extract features from your dataset

The second step involves extracting features from your entire dataset (training and validation). These features will serve for the k-mean clustering proper to WavLM (and [HuBERT](https://arxiv.org/pdf/2106.07447.pdf)), which will serve to generate hidden units.


To remain as close as possible as WavLM's paper, features will be extracted from an already pretrained HuBERT-Base model, as the features will already be quite meaningful. We will use the script "[dump_hubert_feature.py](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/examples/hubert/simple_kmeans/dump_hubert_feature.py#L4)" to do so.

The output will result in one or several files of format **.npy** (containing the features) and **.len** (containing the corresponding length of audio files).

User has to specify :
- The location of the folder containing **train.tsv** and **valid.tsv** 
- The split. It will either be "**train**" or "**valid**" (you will have to apply the script two times).
- The number of shards. As some datasets might take too long to process on a single processing unit, user can split his training into multiple shards of different ranks. To keep the tutorial simple, we will use only **one** shard, of rank **0**.
- The checkpoint location of the [HuBERT-Base model trained on librispeech960h](https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt).
- The **layer** in which you want to extract the features. You might want to choose a layer that suits you best depending on your downstream task (check [WavLM](https://arxiv.org/pdf/2110.13900.pdf)'s section regarding the weight impact of each layer on each downstream task)
- The output location of the features. 

Run : 

```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 ##value between 1 and 12
nshard=1 ##Will generate only one .npy and .len of rank 0
rank=0
feat_dir=output/location/to/.npy_and_.len

##For the train
python location/to/dump_hubert_feature.py ${tsv_dir} train ${hubert_checkpoint} ${layer} ${nshard} ${rank} ${feat_dir}

##For the valid
python location/to/dump_hubert_feature.py ${tsv_dir} valid ${hubert_checkpoint} ${layer} ${nshard} ${rank} ${feat_dir}

```
## STEP 3 : Train a k-mean model based on your generated features

Once we have our features, we need to train a k-mean model in order to generate our clusters. Two main parameters that will matter here is the **number of clusters** and the **ratio of features** that will serve to train this model.
As in the WavLM article, we will use a number of 500 clusters, and a ratio of selected features of 0.1. All the features generated are not used to train this model mainly for memory constraints (but the parameter can of course be changed by the user).

To train this k-mean model, we will use "[learn_kmeans.py](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/examples/hubert/simple_kmeans/learn_kmeans.py#L87)". And we will train a single k-mean model, using the TRAIN split ONLY.

User has to specify :
- The location of the folder containing the **.npy** and **.len** files (the features)
- The number of **shards** and **rank** (has to be the same value as in STEP 2 !)
- The **number of clusters** (500 in WavLM's article)
- The **ratio** of features from the train dataset used (0.1 in WavLM's article)

Run : 

```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 ##value between 1 and 12
nshard=1 ##Will generate only one .npy and .len of rank 0
rank=0
feat_dir=location/to/.npy_and_.len
cluster_nb=500
kmean_train_ratio=0.1

##For the train
python location/to/learn_kmeans.py ${feat_dir} train ${nshard} km_model ${cluster_nb} --percent ${kmean_train_ratio}
```

The script will automatically create a file named "**km_model**" in the same folder as to where the upper bash script was executed.

## STEP 4 : Apply the k-mean model to generate clusters

This part involves applying the trained k-mean model to all our features in order to generate the clusters. The output will result in a file containing the labels,(a number between 0 and the total number of clusters),associated to each of our features. We will have to apply this model to BOTH our train and valid sets.

To do so, we will use "[dump_km_label.py](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/examples/hubert/simple_kmeans/dump_km_label.py#L4)". 

The output will result in several files (depending on the number of rank and shards specified earlier) : **train_${rank}_${nshard}.km** and **valid_${rank}_${nshard}.km** If you used rank=0 and nshard=1, you should only have a **train_0_1.km** and a **valid_0_1.km**.

User has to specify : 
- The location of the folder containing the **.npy** and **.len** files (the features)
- the kmean model (the file named '**km_model**')
- The number of shards and rank (has to be the same value as in STEP 2 !)
- The output location of the labels generated (location of **.km** files generated).

Run :

 ```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 ##value between 1 and 12
nshard=1 ##Will generate only one .npy and .len of rank 0
rank=0
feat_dir=location/to/.npy_and_.len
cluster_nb=500
kmean_train_ratio=0.1
label_location= output/location/to/labels

##For the train
python location/to/dump_km_label.py ${feat_dir} train km_model ${nshard} ${rank} ${label_location}

##For the valid
python location/to/dump_km_label.py ${feat_dir} valid km_model ${nshard} ${rank} ${label_location}
```

## STEP 5 : Merge the km shards into a single file

If you sharded your pretraining preparation, you will end up with several **.km** files for either traininig and validation. This step will allow you to merge all those shards into single files **train.km** and **valid.km** .This step is also necessary in the case where you used rank=0 and nshard=1.

To merge the **.km** shards, you will simply need to execute the following :

 ```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 ##value between 1 and 12
nshard=1 ##Will generate only one .npy and .len of rank 0
rank=0
feat_dir=location/to/.npy_and_.len
cluster_nb=500
kmean_train_ratio=0.1
label_location= location/to/labels

for rank in $(seq 0 $((${nshard} - 1))); do
  cat ${label_location}/train_${rank}_${nshard}.km
done > $(pwd)/${label_location}/train.km

for rank in $(seq 0 $((${nshard} - 1))); do
  cat ${label_location}/valid_${rank}_${nshard}.km
done > $(pwd)/${label_location}/valid.km

 ```

 You should now have two files **train.km** and **valid.km** in your "label_location" folder.

## STEP 6 : Generate the dummy dictionary 

The last step concerns the generation of a dummy dictionary that will contain our total number of clusters. It is a simple **.txt** file which contains numbers from 0 to the total number of clusters.

To generate the dummy dict, simply run : 

 ```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 ##value between 1 and 12
nshard=1 ##Will generate only one .npy and .len of rank 0
rank=0
feat_dir=location/to/.npy_and_.len
cluster_nb=500
kmean_train_ratio=0.1
label_location= location/to/labels

for x in $(seq 0 $((${CLUSTER_NB} - 1))); do
  echo "$x 1"
done >> ${LABEL_LOCATION}/dict.km.txt

 ```

A **dict.km.txt** should be generated near your **train.km** and **valid.km**.

## STEP 7 : Pretraining time !

You made it ! To start the pretraining, you will simply need to use the [WavLM configuration file](https://github.com/SevKod/fairseq/blob/main/examples/hubert/config/pretrain/wavlm_base_librispeech.yaml). There are many parameters that you can tweak to your conveniance there, and they will impact your pretraining. On this section, I will present some important parameters to take into account.

The pretraining supports the use of compute nodes (as these self-supervised models are very demanding in VRAM). 

The WavLM-Base model has been trained using 32 gpus. On the configuration file, the section related to the computational power is called **distributed_training** :

```python
distributed_training:
  ddp_backend: no_c10d
  distributed_backend: 'nccl'
  distributed_world_size: 8
  distributed_port: 29671
  nprocs_per_node: 4
  find_unused_parameters: true
```

There, we can specify the number of gpus per node with **distributed_world_size**, and the number of nodes **nprocs_per_node** reserved. Both of these parameters will result in the use of **distributed_world_size** times **nprocs_per_node** gpus used.

Next is the **task**  (which involves the utterance mixing strategy of WavLM):

```python
task:
  _name: utterance_mixing_pretraining
  data: ???
  label_dir: ???
  labels: ???
  label_rate: ${model.label_rate}
  sample_rate: 16000
  max_sample_size: 160000
  min_sample_size: 80000
  pad_audio: true
  random_crop: true
  normalize: false # must be consistent with extractor
```
The parameters **max_sample_size** and **min_sample_size** will cut each of your audio files to match the selected interval. The **random_crop** will take a random sample size in the **[min_sample_size;max_sample_size]** for audio cropping. If **pad_audio** is set to **true**, then audio segments whose length is inferior to **min_sample_size** will be padded to match that minimal size.

The **dataset** section concerns parameters important parameters such as the (implicit) batch size :
```
dataset:
  num_workers: 6
  max_tokens: 1400000
  skip_invalid_size_inputs_valid_test: true
  validate_interval: 5
  validate_interval_updates: 10000
```
The **num_workers** corresponds to the number of threads (cpu) used for data preprocessing. The parameter **max_tokens** represents the maximal length of audio that can contain a single GPU. If audio files are sampled at 16kHz, this represents 87.5 seconds of audio per GPU. Each batch contains a number of sentences that can be set by the parameter : **max_sentences** . See [here](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/dataclass/configs.py#L464) for more details on all the parameters that you can add.

Next is the criterion : 

```
criterion:
  _name: wavlm
  pred_masked_weight: 1.0
  pred_nomask_weight: 0.0
  loss_weights: [10,]
```
The **pred_masked_weight** and **pred_nomask_weight** correspond to the scalars associated to the loss regarding masked and unmasked prediction (See [HuBERT](https://arxiv.org/pdf/2106.07447.pdf) section regarding loss and especially the parameter **alpha**). By default, the model will only compute loss over masked predictions. See [here](https://github.com/SevKod/fairseq/blob/5b088c7e7d04e0c1a7696bda5ff882c5e600ef05/fairseq/criterions/wavlm_criterion.py#L19) for the total number of parameters.

Finally, we have hyperparameters involving the training directly :

```
optimization:
  max_update: 400000
  #max_update: 800000
  lr: [0.0005]
  clip_norm: 10.0

optimizer:
  _name: adam
  adam_betas: (0.9,0.98)
  adam_eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  _name: polynomial_decay
  warmup_updates: 32000
  #warmup_updates: 0
  #min_lr: 1e-04
```


**max_update** will represent the maximum number of steps until the training stops. In the current case, the model will have a linear learning rate that wil go up until reaching its **warmup_updates** (which corresponds exactly to **8%** of the total training step as mentioned in WavLM's paper). The learning rate will then decay until reaching 0 at exactly the **max_update**. The selected optimizer is **adam**, but a lot of different optimizers can be set in place (see [here](https://github.com/SevKod/fairseq/tree/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/optim) for the list). Same can be said for the learning rate scheduler (see [here](https://github.com/SevKod/fairseq/tree/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/optim/lr_scheduler)).

All aditional parameters regarding the **optimization**, the **optimizer** and the **lr_scheduler** can be respectively found [here](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/dataclass/configs.py#L588), [here](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/optim/adam.py#L25C1-L25C1), and [there](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/optim/lr_scheduler/polynomial_decay_schedule.py).

Finally,  the parameters regarding the WavLM architecture can be found here : 
```
model:
  _name: wavlm
  label_rate: ???
  skip_masked: false
  skip_nomask: false
  mask_prob: 0.80
  extractor_mode: default
  conv_feature_layers: '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'
  final_dim: 256
  encoder_layerdrop: 0.05
  dropout_input: 0.1
  dropout_features: 0.1
  dropout: 0.1
  attention_dropout: 0.1
  feature_grad_mult: 0.1
  untie_final_proj: true
  activation_dropout: 0.0
  
  #Gated Relative Position Bias
  #relative_position_embedding: true
  #gru_rel_pos: true

  #Change the architecture
  #encoder_attention_heads: 16
  #encoder_layers: 24
  #encoder_embed_dim: 1024
  #encoder_ffn_embed_dim: 4096
  #final_dim: 768
  #layer_norm_first: True

```

Additional parameters can be specified here such as the activation of the **Gated Relative Position Bias** or not, or the number of transformer encoders...

**Gated Relative Position Bias** is deactivated by default in the configuration file, as the training is abnormally longer and seems to produce neglectable performance improvements on multiple downstream tasks... Though, it can be activated by passing both **relative_position_embedding** and **gru_rel_pos** to **true**. All the parameters that you can tweak can be found [here](https://github.com/SevKod/fairseq/blob/1a2ef433ae21b6bcc4d7d264053e73479f7372ac/fairseq/models/wavlm/wavlm.py#L67).

Regarding checkpoint saving and studying the pretraining progression : 

 ```python
checkpoint:
  save_interval_updates: 25000
  keep_interval_updates: 1
  no_epoch_checkpoints: true
 ```
Parameter **keep_interval_updates** will save a checkpoint at each epoch, wich can allow you to study the improvement of your pretrained model on a certain downstream task. Parameters for the checkpoint can be found [here ](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq/dataclass/configs.py#L643).

Regarding the watch of the training, **Tensorboard** is used for visualization. 
```
hydra:
  job:
    config:
      override_dirname:
        kv_sep: '-'
        item_sep: '__'
        exclude_keys:
          - run
          - task.data
          - task.label_dir
  run:
    dir: 32gpu
  sweep:
    dir: ???
    subdir: ${hydra.job.config_name}__${hydra.job.override_dirname}
```
You can simply change the **dir:** folder name in the **run:** section to name your Tensorboard (and checkpoint) folder.


Pretraining will require the use of  "[hydra_train.py](https://github.com/SevKod/fairseq/blob/31fba013a070eaff69dec8642e68e7134d60ab0f/fairseq_cli/hydra_train.py#L4)".

When you are ready to start the pretraining of WavLM, run the following : 

 ```python
data_location=unique/location/to/audio
tsv_dir=location/of/tsv
format=wav_or_flac
ratio=0.01 
hubert_checkpoint=location/to/hubert_base_ls960.pt
layer=5 
nshard=1 
rank=0
feat_dir=location/to/.npy_and_.len
cluster_nb=500
kmean_train_ratio=0.1
label_location= location/to/labels

python fairseq/fairseq_cli/hydra_train.py \
 --config-dir fairseq/examples/hubert/config/pretrain \
 --config-name wavlm_base_librispeech \
 task.data=$(pwd)/${tsv_dir} task.label_dir=$(pwd)/label task.labels='["km"]' model.label_rate=50
```

The pretraining should start, and a tensorboard directory should be created at the specified path.


## STEP 8 : Extract your audio representations !

Great ! You have completed the pretraining, and should now posess a **.pt** file containing the weights of your pretrained model. 

In order to simplify as much as possible the feature extraction, and be completely independant of any fairseq dependency, converting your checkpoint is preferable. 

In this short section, we will use the script **convert_fairseq_torch.py** to create a new checkpoint that will contain the **name** of our model (**WavLMModel**), its **configuration**, and of course, the **state_dict** containing the weights ! 

Simply run : 

 ```python
fairseq_pt=location/to/fairseq/pt

python convert_fairseq_torch.py ${fairseq_pt}
```

A **.ckpt** carrying the same name as your fairseq checkpoint should be created.

Then, to extract representations, use the following PyTorch script :

 ```python
import torch 
import torchaudio
from torchaudio.models.wav2vec2 import wav2vec2_model

ckpt_name = 'your_new_ckpt'

ckpt = torch.load(ckpt_name)
model = wav2vec2_model(**ckpt['config'])
model.load_state_dict(ckpt['state_dict'])
model.eval() #Your model is ready to be used !

#Specify in advance the layer you want to extract your representations from (0 to 11), to truncate the unused encoder layers
max_layer = 4
#Extract features using :

with torch.no_grad():
  features, _ = model.extract_features(waveforms,None,max_layer+1)
```

feat = features[4]
